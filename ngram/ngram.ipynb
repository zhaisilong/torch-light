{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set flags / seeds\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "test_sentence = \"\"\"n-gram models are widely used in statistical natural\n",
    "language processing . In speech recognition , phonemes and sequences of\n",
    "phonemes are modeled using a n-gram distribution . For parsing , words\n",
    "are modeled such that each n-gram is composed of n words . For language\n",
    "identification , sequences of characters / graphemes ( letters of the alphabet\n",
    ") are modeled for different languages For sequences of characters ,\n",
    "the 3-grams ( sometimes referred to as \" trigrams \" ) that can be\n",
    "generated from \" good morning \" are \" goo \" , \" ood \" , \" od \" , \" dm \",\n",
    "\" mo \" , \" mor \" and so forth , counting the space character as a gram\n",
    "( sometimes the beginning and end of a text are modeled explicitly , adding\n",
    "\" __g \" , \" _go \" , \" ng_ \" , and \" g__ \" ) . For sequences of words ,\n",
    "the trigrams that can be generated from \" the dog smelled like a skunk \"\n",
    "are \" # the dog \" , \" the dog smelled \" , \" dog smelled like \", \" smelled\n",
    "like a \" , \" like a skunk \" and \" a skunk # \" .\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [([test_sentence[i], test_sentence[i+1]],\n",
    "            test_sentence[i+2]) for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=16, context_size=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.l1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.l2 = nn.Linear(128, vocab_size)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view(1, -1)\n",
    "        out = F.relu(self.l1(embeds))\n",
    "        out = self.l2(out)\n",
    "        log_probs = F.log_softmax(out, dim=-1)\n",
    "        return log_probs\n",
    "\n",
    "    # 初始化参数\n",
    "    def _init_weight(self, scope=0.1):\n",
    "        # 这里要使用 data 才能修改值\n",
    "        self.embeddings.weight.data.uniform_(-scope, scope)\n",
    "        self.l1.weight.data.uniform_(0, scope)\n",
    "        self.l1.bias.data.fill_(0)\n",
    "        self.l2.weight.data.uniform_(0, scope)\n",
    "        self.l2.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "model = NGram(len(vocab))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908.8380126953125\n",
      "798.0969848632812\n",
      "744.8796997070312\n",
      "707.7742919921875\n",
      "673.3292846679688\n",
      "637.6712036132812\n",
      "599.3861083984375\n",
      "557.2484130859375\n",
      "510.922119140625\n",
      "461.93707275390625\n",
      "412.8521728515625\n",
      "367.5153503417969\n",
      "327.61126708984375\n",
      "290.9803466796875\n",
      "258.08612060546875\n",
      "228.65982055664062\n",
      "203.10037231445312\n",
      "180.83921813964844\n",
      "163.3822784423828\n",
      "148.52215576171875\n",
      "137.2045135498047\n",
      "127.42076110839844\n",
      "119.11690521240234\n",
      "112.48844146728516\n",
      "106.7303237915039\n",
      "102.3620834350586\n",
      "98.2813720703125\n",
      "95.12329864501953\n",
      "92.1830825805664\n",
      "90.16474151611328\n",
      "88.42735290527344\n",
      "86.56852722167969\n",
      "84.93019104003906\n",
      "83.75806427001953\n",
      "82.68902587890625\n",
      "81.57109069824219\n",
      "80.80103302001953\n",
      "79.99109649658203\n",
      "79.34346771240234\n",
      "78.94111633300781\n",
      "78.17378997802734\n",
      "77.45096588134766\n",
      "77.22769927978516\n",
      "76.73011016845703\n",
      "76.53297424316406\n",
      "76.14115905761719\n",
      "75.9480972290039\n",
      "75.77069854736328\n",
      "75.21294403076172\n",
      "75.12097930908203\n",
      "74.6357421875\n",
      "74.49486541748047\n",
      "74.4771499633789\n",
      "74.07783508300781\n",
      "73.84907531738281\n",
      "73.54241180419922\n",
      "73.71871185302734\n",
      "73.14044189453125\n",
      "72.87731170654297\n",
      "72.84506225585938\n",
      "72.80982971191406\n",
      "72.46477508544922\n",
      "72.15264892578125\n",
      "72.02534484863281\n",
      "71.93077850341797\n",
      "71.67707824707031\n",
      "71.80088806152344\n",
      "71.45283508300781\n",
      "71.42878723144531\n",
      "71.11648559570312\n",
      "70.92507934570312\n",
      "71.25226593017578\n",
      "70.73674774169922\n",
      "70.6313705444336\n",
      "70.55267333984375\n",
      "70.25273895263672\n",
      "70.28115844726562\n",
      "70.14191436767578\n",
      "69.89913177490234\n",
      "69.8108139038086\n",
      "69.78158569335938\n",
      "69.80877685546875\n",
      "69.4084701538086\n",
      "69.32423400878906\n",
      "69.30533599853516\n",
      "69.21699523925781\n",
      "69.2840347290039\n",
      "68.91928100585938\n",
      "68.90849304199219\n",
      "68.81045532226562\n",
      "68.5576171875\n",
      "68.47969818115234\n",
      "68.50568389892578\n",
      "68.36933135986328\n",
      "68.25149536132812\n",
      "68.41940307617188\n",
      "68.19648742675781\n",
      "67.89747619628906\n",
      "68.14136505126953\n",
      "68.0213623046875\n",
      "widely + used = in\n",
      "and + so = forth\n",
      "are + modeled = explicitly\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "        context_idxs = list(map(lambda w: word2idx[w], context))\n",
    "        context_var = torch.LongTensor(context_idxs)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        log_probs = model(context_var)\n",
    "        loss = criterion(log_probs, torch.LongTensor([word2idx[target]]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    print(total_loss.item())\n",
    "\n",
    "model.eval()\n",
    "def predict(context):\n",
    "    context_idxs = list(map(lambda w: word2idx[w], context))\n",
    "    context_var = torch.LongTensor(context_idxs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict = model(context_var)\n",
    "        index = (torch.max(predict, 1)[1]).tolist()[0]\n",
    "        return idx2word[index]\n",
    "\n",
    "\n",
    "for context in [[\"widely\", \"used\"], [\"and\", \"so\"], [\"are\", \"modeled\"]]:\n",
    "    print(\"{} + {} = {}\".format(context[0], context[1], predict(context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96648026e1e6821349af1ac8e0551e647cc64c32425bd6c61aa6fe9eaa637e4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
